{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADRs extraction using Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-796fbc5e7f06>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_text['index'] = data_text.index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Mirena_IUD.csv', encoding='cp1252',error_bad_lines=False);\n",
    "data_text = data[['Reviews']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I got the Mirena in Feb 2020. The insertion wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is 2nd time I have had the Mirena. Total ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The biggest joke known to mankind. Doctor gave...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I’ve had mirena for exactly one month now and ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have a history of gushing, painful periods w...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews  index\n",
       "0  I got the Mirena in Feb 2020. The insertion wa...      0\n",
       "1  This is 2nd time I have had the Mirena. Total ...      1\n",
       "2  The biggest joke known to mankind. Doctor gave...      2\n",
       "3  I’ve had mirena for exactly one month now and ...      3\n",
       "4  I have a history of gushing, painful periods w...      4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "\n",
    "#preprocess will remove the stop words & word_length<4 words append the word in it's stemmed form\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Had', 'it', 'for', 'almost', 'a', 'year.', 'Bloated,', 'hair', 'thinning', 'and', 'would', 'get', 'back', 'cramps', 'all', 'the', 'time.', 'Has', 'it', 'removed', 'and', 'had', 'instant', 'relief!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['year', 'bloat', 'hair', 'thin', 'cramp', 'time', 'remov', 'instant', 'relief']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 1008].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['Reviews'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [mirena, insert, wasn, pleasant, mean, like, i...\n",
       "1    [time, mirena, total, year, total, issu, surf,...\n",
       "2    [biggest, joke, know, mankind, doctor, give, s...\n",
       "3    [mirena, exact, month, complet, nightmar, doct...\n",
       "4    [histori, gush, pain, period, includ, vomit, l...\n",
       "5    [mirena, insert, decemb, gyna, recommend, help...\n",
       "6    [birth, control, devic, ruin, life, total, mon...\n",
       "7    [mirena, month, stand, anymor, constant, cramp...\n",
       "8    [facial, hair, huge, acn, pimpl, destroy, sugg...\n",
       "9    [recommend, mirena, eas, endometriosi, pain, b...\n",
       "Name: Reviews, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3180"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 abl\n",
      "1 appetit\n",
      "2 attent\n",
      "3 bodi\n",
      "4 boyfriend\n",
      "5 break\n",
      "6 chang\n",
      "7 chest\n",
      "8 contract\n",
      "9 control\n",
      "10 couldn\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems(): #key & value in dictionary\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1028"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "#bow_corpus[117]\n",
    "len(bow_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 101 (\"bleed\") appears 1 time.\n",
      "Word 113 (\"tender\") appears 1 time.\n",
      "Word 141 (\"mood\") appears 1 time.\n",
      "Word 150 (\"swing\") appears 1 time.\n",
      "Word 241 (\"breast\") appears 1 time.\n",
      "Word 441 (\"nauseous\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_801 = bow_corpus[801]\n",
    "\n",
    "for i in range(len(bow_doc_801)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_801[i][0], \n",
    "                                                     dictionary[bow_doc_801[i][0]], \n",
    "                                                     bow_doc_801[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.24143905390274553),\n",
      " (1, 0.08054150798236333),\n",
      " (2, 0.14840155449816345),\n",
      " (3, 0.14439943615678255),\n",
      " (4, 0.20000002210824572),\n",
      " (5, 0.16542722110881483),\n",
      " (6, 0.1092239770460668),\n",
      " (7, 0.20835860919445964),\n",
      " (8, 0.10669445514597896),\n",
      " (9, 0.14702469473774815),\n",
      " (10, 0.11384794784517635),\n",
      " (11, 0.12780007229918525),\n",
      " (12, 0.11658598296119647),\n",
      " (13, 0.14439943615678255),\n",
      " (14, 0.13738475821151322),\n",
      " (15, 0.15282569107032426),\n",
      " (16, 0.050746424645124945),\n",
      " (17, 0.06980371910435576),\n",
      " (18, 0.04325053535877227),\n",
      " (19, 0.14074578836728166),\n",
      " (20, 0.12530485550847062),\n",
      " (21, 0.10259425420509913),\n",
      " (22, 0.083121938481233),\n",
      " (23, 0.11929598518446949),\n",
      " (24, 0.1675810501612803),\n",
      " (25, 0.08341869208743972),\n",
      " (26, 0.18583715226261865),\n",
      " (27, 0.09500516419468213),\n",
      " (28, 0.11593495502870105),\n",
      " (29, 0.15955855278655381),\n",
      " (30, 0.1498250962650667),\n",
      " (31, 0.1675810501612803),\n",
      " (32, 0.3495942431771688),\n",
      " (33, 0.21338891029195792),\n",
      " (34, 0.1775071238118574),\n",
      " (35, 0.10992608470469993),\n",
      " (36, 0.1269517513153513),\n",
      " (37, 0.07606179170584555),\n",
      " (38, 0.0991251781507439),\n",
      " (39, 0.14702469473774815),\n",
      " (40, 0.07133878938439714),\n",
      " (41, 0.07458829102702746),\n",
      " (42, 0.12909612218539673),\n",
      " (43, 0.15556544423715649),\n",
      " (44, 0.08166883637483995),\n",
      " (45, 0.14869531471391556),\n",
      " (46, 0.09054180252811254),\n",
      " (47, 0.047712827735931156),\n",
      " (48, 0.07224184484547197),\n",
      " (49, 0.15955855278655381)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=10, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.033*\"year\" + 0.027*\"effect\" + 0.023*\"week\" + 0.020*\"experi\" + 0.019*\"take\" + 0.018*\"pill\" + 0.016*\"go\" + 0.014*\"get\" + 0.013*\"doctor\" + 0.012*\"caus\"\n",
      "Topic: 1 \n",
      "Words: 0.028*\"year\" + 0.026*\"felt\" + 0.021*\"experi\" + 0.021*\"like\" + 0.018*\"time\" + 0.017*\"take\" + 0.013*\"pill\" + 0.013*\"work\" + 0.012*\"doctor\" + 0.012*\"go\"\n",
      "Topic: 2 \n",
      "Words: 0.032*\"like\" + 0.025*\"feel\" + 0.021*\"experi\" + 0.019*\"felt\" + 0.018*\"want\" + 0.017*\"take\" + 0.017*\"hour\" + 0.017*\"go\" + 0.016*\"hurt\" + 0.015*\"birth\"\n",
      "Topic: 3 \n",
      "Words: 0.043*\"remov\" + 0.024*\"doctor\" + 0.019*\"start\" + 0.017*\"have\" + 0.017*\"bleed\" + 0.016*\"year\" + 0.016*\"hair\" + 0.015*\"migrain\" + 0.014*\"time\" + 0.013*\"want\"\n",
      "Topic: 4 \n",
      "Words: 0.035*\"week\" + 0.033*\"go\" + 0.031*\"bleed\" + 0.028*\"remov\" + 0.027*\"year\" + 0.021*\"feel\" + 0.021*\"day\" + 0.020*\"time\" + 0.017*\"think\" + 0.015*\"spot\"\n",
      "Topic: 5 \n",
      "Words: 0.024*\"feel\" + 0.024*\"like\" + 0.020*\"spot\" + 0.019*\"weight\" + 0.018*\"control\" + 0.017*\"birth\" + 0.017*\"year\" + 0.016*\"gain\" + 0.014*\"go\" + 0.014*\"week\"\n",
      "Topic: 6 \n",
      "Words: 0.043*\"review\" + 0.025*\"go\" + 0.024*\"read\" + 0.020*\"like\" + 0.020*\"cervix\" + 0.018*\"feel\" + 0.018*\"take\" + 0.018*\"doctor\" + 0.018*\"procedur\" + 0.017*\"minut\"\n",
      "Topic: 7 \n",
      "Words: 0.034*\"gain\" + 0.033*\"weight\" + 0.019*\"tell\" + 0.019*\"week\" + 0.019*\"bodi\" + 0.018*\"feel\" + 0.017*\"year\" + 0.016*\"start\" + 0.015*\"like\" + 0.015*\"remov\"\n",
      "Topic: 8 \n",
      "Words: 0.036*\"acn\" + 0.023*\"take\" + 0.021*\"littl\" + 0.018*\"pill\" + 0.018*\"say\" + 0.015*\"terribl\" + 0.015*\"year\" + 0.014*\"day\" + 0.014*\"go\" + 0.014*\"worth\"\n",
      "Topic: 9 \n",
      "Words: 0.055*\"bleed\" + 0.041*\"year\" + 0.033*\"control\" + 0.033*\"birth\" + 0.023*\"heavi\" + 0.021*\"spot\" + 0.020*\"day\" + 0.020*\"have\" + 0.018*\"stop\" + 0.016*\"experi\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=15, id2word=dictionary, passes=50, workers=50)\n",
    "#no of workers used for parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.012*\"remov\" + 0.012*\"gain\" + 0.012*\"weight\" + 0.011*\"bleed\" + 0.010*\"acn\" + 0.010*\"mood\" + 0.009*\"depress\" + 0.009*\"wait\" + 0.009*\"lose\" + 0.009*\"hair\"\n",
      "Topic: 1 Word: 0.052*\"cyst\" + 0.036*\"ovarian\" + 0.035*\"attack\" + 0.033*\"panic\" + 0.032*\"lot\" + 0.030*\"multipl\" + 0.029*\"migrain\" + 0.028*\"anxieti\" + 0.027*\"ovari\" + 0.017*\"believ\"\n",
      "Topic: 2 Word: 0.068*\"surgeri\" + 0.036*\"pregnant\" + 0.028*\"hospit\" + 0.028*\"cours\" + 0.024*\"extrem\" + 0.023*\"coil\" + 0.023*\"write\" + 0.021*\"remov\" + 0.021*\"appar\" + 0.019*\"problem\"\n",
      "Topic: 3 Word: 0.117*\"updat\" + 0.031*\"appar\" + 0.026*\"sweat\" + 0.012*\"call\" + 0.011*\"appoint\" + 0.009*\"heat\" + 0.009*\"suck\" + 0.009*\"wasnt\" + 0.008*\"ask\" + 0.008*\"painless\"\n",
      "Topic: 4 Word: 0.009*\"bleed\" + 0.009*\"year\" + 0.009*\"week\" + 0.009*\"spot\" + 0.008*\"like\" + 0.008*\"birth\" + 0.008*\"day\" + 0.008*\"experi\" + 0.007*\"control\" + 0.007*\"go\"\n",
      "Topic: 5 Word: 0.118*\"bear\" + 0.025*\"sharp\" + 0.017*\"stay\" + 0.014*\"stab\" + 0.013*\"scare\" + 0.012*\"women\" + 0.012*\"child\" + 0.008*\"work\" + 0.002*\"drug\" + 0.002*\"handl\"\n",
      "Topic: 6 Word: 0.037*\"bleed\" + 0.025*\"major\" + 0.017*\"issu\" + 0.017*\"develop\" + 0.016*\"alot\" + 0.015*\"moodi\" + 0.014*\"irregular\" + 0.011*\"uterus\" + 0.011*\"dont\" + 0.008*\"start\"\n",
      "Topic: 7 Word: 0.021*\"zero\" + 0.021*\"medic\" + 0.018*\"love\" + 0.018*\"person\" + 0.016*\"increas\" + 0.016*\"worst\" + 0.015*\"recommend\" + 0.014*\"suppos\" + 0.014*\"life\" + 0.014*\"today\"\n",
      "Topic: 8 Word: 0.101*\"pelvic\" + 0.088*\"explain\" + 0.043*\"great\" + 0.026*\"recommend\" + 0.022*\"love\" + 0.022*\"problem\" + 0.015*\"pull\" + 0.014*\"twice\" + 0.013*\"birth\" + 0.013*\"control\"\n",
      "Topic: 9 Word: 0.125*\"disappear\" + 0.123*\"daughter\" + 0.030*\"away\" + 0.014*\"stab\" + 0.010*\"ladi\" + 0.006*\"bare\" + 0.005*\"enjoy\" + 0.002*\"anxieti\" + 0.002*\"depress\" + 0.002*\"horribl\"\n",
      "Topic: 10 Word: 0.017*\"review\" + 0.016*\"cervix\" + 0.015*\"bare\" + 0.015*\"risk\" + 0.015*\"pinch\" + 0.014*\"believ\" + 0.014*\"dilat\" + 0.013*\"bearabl\" + 0.013*\"procedur\" + 0.013*\"felt\"\n",
      "Topic: 11 Word: 0.023*\"normal\" + 0.018*\"overal\" + 0.017*\"easi\" + 0.015*\"nauseous\" + 0.015*\"flow\" + 0.015*\"coupl\" + 0.015*\"call\" + 0.014*\"partner\" + 0.014*\"problem\" + 0.014*\"day\"\n",
      "Topic: 12 Word: 0.034*\"stomach\" + 0.024*\"liter\" + 0.020*\"keep\" + 0.016*\"give\" + 0.015*\"shoot\" + 0.015*\"sever\" + 0.014*\"leav\" + 0.013*\"thing\" + 0.013*\"terribl\" + 0.011*\"get\"\n",
      "Topic: 13 Word: 0.104*\"liner\" + 0.100*\"panti\" + 0.044*\"case\" + 0.042*\"relax\" + 0.024*\"concern\" + 0.018*\"breastfe\" + 0.012*\"decreas\" + 0.012*\"anymor\" + 0.011*\"drive\" + 0.010*\"fibroid\"\n",
      "Topic: 14 Word: 0.034*\"plus\" + 0.013*\"open\" + 0.012*\"hear\" + 0.011*\"loss\" + 0.010*\"hair\" + 0.009*\"control\" + 0.009*\"children\" + 0.007*\"want\" + 0.002*\"suffer\" + 0.002*\"face\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chill',\n",
       " 'itch',\n",
       " 'mood',\n",
       " 'swing',\n",
       " 'abnorm',\n",
       " 'bleed',\n",
       " 'cramp',\n",
       " 'nauseous',\n",
       " 'breast',\n",
       " 'tender',\n",
       " 'pain']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[801]  #750,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.871407687664032\t \n",
      "Topic: 0.035*\"week\" + 0.033*\"go\" + 0.031*\"bleed\" + 0.028*\"remov\" + 0.027*\"year\" + 0.021*\"feel\" + 0.021*\"day\" + 0.020*\"time\" + 0.017*\"think\" + 0.015*\"spot\" + 0.015*\"stop\" + 0.014*\"have\" + 0.013*\"get\" + 0.012*\"know\" + 0.012*\"constant\" + 0.010*\"cycl\" + 0.010*\"bodi\" + 0.010*\"blood\" + 0.010*\"light\" + 0.009*\"recommend\"\n",
      "\n",
      "Score: 0.014289222657680511\t \n",
      "Topic: 0.024*\"feel\" + 0.024*\"like\" + 0.020*\"spot\" + 0.019*\"weight\" + 0.018*\"control\" + 0.017*\"birth\" + 0.017*\"year\" + 0.016*\"gain\" + 0.014*\"go\" + 0.014*\"week\" + 0.013*\"lose\" + 0.013*\"bleed\" + 0.013*\"thing\" + 0.012*\"have\" + 0.012*\"time\" + 0.011*\"day\" + 0.011*\"horribl\" + 0.011*\"heavi\" + 0.011*\"depress\" + 0.011*\"experi\"\n",
      "\n",
      "Score: 0.014289028011262417\t \n",
      "Topic: 0.033*\"year\" + 0.027*\"effect\" + 0.023*\"week\" + 0.020*\"experi\" + 0.019*\"take\" + 0.018*\"pill\" + 0.016*\"go\" + 0.014*\"get\" + 0.013*\"doctor\" + 0.012*\"caus\" + 0.012*\"remov\" + 0.011*\"have\" + 0.011*\"gain\" + 0.011*\"extrem\" + 0.011*\"felt\" + 0.011*\"weight\" + 0.011*\"work\" + 0.010*\"hormon\" + 0.010*\"recommend\" + 0.009*\"like\"\n",
      "\n",
      "Score: 0.014288444072008133\t \n",
      "Topic: 0.032*\"like\" + 0.025*\"feel\" + 0.021*\"experi\" + 0.019*\"felt\" + 0.018*\"want\" + 0.017*\"take\" + 0.017*\"hour\" + 0.017*\"go\" + 0.016*\"hurt\" + 0.015*\"birth\" + 0.015*\"have\" + 0.014*\"time\" + 0.014*\"year\" + 0.014*\"get\" + 0.013*\"control\" + 0.013*\"doctor\" + 0.013*\"tell\" + 0.011*\"coupl\" + 0.011*\"recommend\" + 0.009*\"work\"\n",
      "\n",
      "Score: 0.014288267120718956\t \n",
      "Topic: 0.043*\"remov\" + 0.024*\"doctor\" + 0.019*\"start\" + 0.017*\"have\" + 0.017*\"bleed\" + 0.016*\"year\" + 0.016*\"hair\" + 0.015*\"migrain\" + 0.014*\"time\" + 0.013*\"want\" + 0.013*\"day\" + 0.012*\"like\" + 0.012*\"sever\" + 0.012*\"life\" + 0.011*\"feel\" + 0.011*\"think\" + 0.011*\"depress\" + 0.011*\"hormon\" + 0.011*\"effect\" + 0.011*\"experi\"\n",
      "\n",
      "Score: 0.014288068749010563\t \n",
      "Topic: 0.036*\"acn\" + 0.023*\"take\" + 0.021*\"littl\" + 0.018*\"pill\" + 0.018*\"say\" + 0.015*\"terribl\" + 0.015*\"year\" + 0.014*\"day\" + 0.014*\"go\" + 0.014*\"worth\" + 0.013*\"child\" + 0.013*\"migrain\" + 0.013*\"hour\" + 0.012*\"scar\" + 0.012*\"pregnanc\" + 0.012*\"have\" + 0.011*\"birth\" + 0.010*\"light\" + 0.010*\"love\" + 0.010*\"away\"\n",
      "\n",
      "Score: 0.014287954196333885\t \n",
      "Topic: 0.055*\"bleed\" + 0.041*\"year\" + 0.033*\"control\" + 0.033*\"birth\" + 0.023*\"heavi\" + 0.021*\"spot\" + 0.020*\"day\" + 0.020*\"have\" + 0.018*\"stop\" + 0.016*\"experi\" + 0.016*\"effect\" + 0.014*\"week\" + 0.014*\"recommend\" + 0.014*\"time\" + 0.012*\"pill\" + 0.011*\"remov\" + 0.011*\"go\" + 0.009*\"pregnanc\" + 0.009*\"life\" + 0.009*\"place\"\n",
      "\n",
      "Score: 0.014287530444562435\t \n",
      "Topic: 0.028*\"year\" + 0.026*\"felt\" + 0.021*\"experi\" + 0.021*\"like\" + 0.018*\"time\" + 0.017*\"take\" + 0.013*\"pill\" + 0.013*\"work\" + 0.012*\"doctor\" + 0.012*\"go\" + 0.011*\"think\" + 0.011*\"spot\" + 0.011*\"remov\" + 0.011*\"get\" + 0.010*\"day\" + 0.010*\"children\" + 0.010*\"procedur\" + 0.010*\"life\" + 0.009*\"thing\" + 0.009*\"bleed\"\n",
      "\n",
      "Score: 0.014287137426435947\t \n",
      "Topic: 0.034*\"gain\" + 0.033*\"weight\" + 0.019*\"tell\" + 0.019*\"week\" + 0.019*\"bodi\" + 0.018*\"feel\" + 0.017*\"year\" + 0.016*\"start\" + 0.015*\"like\" + 0.015*\"remov\" + 0.013*\"effect\" + 0.013*\"doctor\" + 0.012*\"bloat\" + 0.012*\"thing\" + 0.012*\"felt\" + 0.012*\"pound\" + 0.010*\"go\" + 0.009*\"day\" + 0.009*\"lose\" + 0.009*\"better\"\n",
      "\n",
      "Score: 0.014286626130342484\t \n",
      "Topic: 0.043*\"review\" + 0.025*\"go\" + 0.024*\"read\" + 0.020*\"like\" + 0.020*\"cervix\" + 0.018*\"feel\" + 0.018*\"take\" + 0.018*\"doctor\" + 0.018*\"procedur\" + 0.017*\"minut\" + 0.015*\"today\" + 0.014*\"good\" + 0.014*\"say\" + 0.013*\"littl\" + 0.013*\"experi\" + 0.013*\"felt\" + 0.012*\"wasn\" + 0.012*\"second\" + 0.012*\"year\" + 0.011*\"bleed\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[801]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7238268852233887\t \n",
      "Topic: 0.012*\"remov\" + 0.012*\"gain\" + 0.012*\"weight\" + 0.011*\"bleed\" + 0.010*\"acn\" + 0.010*\"mood\" + 0.009*\"depress\" + 0.009*\"wait\" + 0.009*\"lose\" + 0.009*\"hair\"\n",
      "\n",
      "Score: 0.15235751867294312\t \n",
      "Topic: 0.023*\"normal\" + 0.018*\"overal\" + 0.017*\"easi\" + 0.015*\"nauseous\" + 0.015*\"flow\" + 0.015*\"coupl\" + 0.015*\"call\" + 0.014*\"partner\" + 0.014*\"problem\" + 0.014*\"day\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[801]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9307525753974915\t Topic: 0.024*\"feel\" + 0.024*\"like\" + 0.020*\"spot\" + 0.019*\"weight\" + 0.018*\"control\" + 0.017*\"birth\" + 0.017*\"year\" + 0.016*\"gain\" + 0.014*\"go\" + 0.014*\"week\" + 0.013*\"lose\" + 0.013*\"bleed\" + 0.013*\"thing\" + 0.012*\"have\" + 0.012*\"time\"\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "\n",
    "unseen_document = 'i gained weight the first month with no change to my diet i lose hair constantly i seem to irratated all day every day i have horrible headches crying spells after a few months i couldnt feel the strings anymore its just not for everyone'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
